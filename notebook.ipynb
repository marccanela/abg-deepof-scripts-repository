{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepOF notebook for Busquets-Garcia Lab\n",
    "Welcome to this notebook, where you'll find all the code necessary to analyze your DeepLabCut (DLC) output with DeepOF. Don't worry! You don't need notions of coding with Python, as you'll need to run the cells of this notebook. Just keep reading, and I'll guide you through the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages and directories\n",
    "First, run the following cell to import the necessary packages for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:16:31.994364: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-15 10:16:31.994404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-15 10:16:31.994444: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-15 10:16:32.003370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-15 10:16:32.682381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-15 10:16:40.692444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:40.721428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:40.721669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:40.723906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:40.724099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:40.724273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:41.193086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:41.193304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:41.193489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 10:16:41.193625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13434 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "/home/sie/anaconda3/envs/deepof/lib/python3.9/site-packages/ot/backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add your directories. Modify the following to fit the directories where you store your files:\n",
    "NOTE: run the corrected videos with the DLC files of the corrected videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_output = '/home/sie/Desktop/Marc/Unsupervised/'\n",
    "directory_dlc = '//FOLDER/becell/Lab Projects/ERCstG_HighMemory/Data/Marc/1) SOC/2023-10 - Alzheimer/Females/DeepOF/dlc/'\n",
    "directory_videos = '//FOLDER/becell/Lab Projects/ERCstG_HighMemory/Data/Marc/1) SOC/2023-10 - Alzheimer/Females/DeepOF/corrected/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your DeepOF project folder\n",
    "Now that you have all set, it's time to start your analysis. Just run the following cell to create a folder with your DeepOF project (where you'll perform your study). But before, take into consideration:\n",
    "- To draw an area, click on the corners of the arena. Press **d** to delete and **q** once you have finished.\n",
    "- The first edge that you draw will be used to escalate from pixels to millimeters. The default value is 200 mm, corresponding to the longest edge of the polybox.\n",
    "- We will perform the arena detection semi-automatically. You'll have to draw the arena once, and the rest will be predicted based on your first attempt. The program will create a folder named *Arena_detection* to store samples of the automatic detection so you can check if everything went smoothly.\n",
    "- If you want to perform a manual drawing in each video (it may be tedious!), you can do it by switching **manual=False** to **manual=True** in the function below.\n",
    "- If you have a problem drawing the arena of a specific video, don't worry! Note the number of the video causing problems, draw any shape, and go on. After the analysis is completed, run the next cell indicating the video's name and re-draw the arena's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_script import creating_deepof_project\n",
    "my_deepof_project = creating_deepof_project(directory_output, directory_dlc, directory_videos, manual=False, scale=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To re-draw the shape of an arena\n",
    "my_deepof_project.edit_arenas(videos=['20230728_Marc_ERC SOC S1_Males_box ef_05_01_1',\n",
    "                                      '20230728_Marc_ERC SOC S2_Males_box ab_04_01_1',\n",
    "                                      '20230728_Marc_ERC SOC S2_Males_box ab_06_01_1',\n",
    "                                      '20230728_Marc_ERC SOC S2_Males_box cd_04_01_1'\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may add some conditions to your experiment (e.g., a column named *protocol* in which you indicate the group to which the video belongs). To do so, first, you must create a CSV file (you can do that in Excel and then export it to CSV) and store it in your _directory_output_. You cave an example named ``conditions.csv`` in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project.load_exp_conditions(directory_output + \"conditions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (master_script.py, line 96)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/deepof/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3548\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\n\u001b[0;31m    from master_script import check_conditions\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/Marc/GitHub/abg-deepof-scripts-repository/master_script.py:96\u001b[0;36m\u001b[0m\n\u001b[0;31m    def train_model_function(my_deepof_project, graph_preprocessed_coords, adj_matrix, pre_trained)\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Optional: in case you want to check that your conditions have been correctly imported:\n",
    "from master_script import check_conditions\n",
    "check_conditions(directory_output, my_deepof_project, col_name='protocol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a supervised analysis\n",
    "With the supervised analysis, you can predict different pre-defined behaviors. We start supposing that you created a DeepOF project (that is to say, that you ran the cells from the section above). In case you have a DeepOF project and you want to load it, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load_project(directory_output + \"deepof_tutorial_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded with a DeepOF project, it's time to run the supervised analysis! For the sake of time, we'll store the output from the supervised analysis in a pickle file named **supervised_annotation.pkl**. Then, you can load it in the future and start working with it immediately without re-doing the analysis. In summary, to perform a new supervised analysis, run the first cell; to load an existing one, the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a new supervised analysis\n",
    "supervised_annotation = my_deepof_project.supervised_annotation()\n",
    "with open(directory_output + 'supervised_annotation.pkl', 'wb') as file:\n",
    "    pickle.dump(supervised_annotation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, open an existing supervised analysis\n",
    "with open(directory_output + 'supervised_annotation.pkl', 'rb') as file:\n",
    "    supervised_annotation = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that we have other data sources, we might think it would be interesting to merge all of them. Running the following cell, we can combine the output of Júlia's and Rémi's script for immobility analysis into the data of DeepOF. You must enter a folder directory where you store the pickle files obtained after the immobility analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_script import update_supervised_annotation_with_immobility\n",
    "\n",
    "directory_path = '//FOLDER/becell/Lab Projects/ERCstG_HighMemory/Data/Marc/1) SOC/2023-09 - Young males/DeepOF/Data/pickles/'\n",
    "update_supervised_annotation_with_immobility(supervised_annotation, directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform an unsupervised analysis\n",
    "With an unsupervised analysis, you can identify patterns in your data without labeling or a pre-existing hypothesis. Therefore, you can use it to explore your data and discover new relationships you wouldn't have noticed! First, we'll preprocess our data; DeepOF will calculate the centered and aligned coordinates, speeds, and distances corresponding between the animal parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_script import graph_dataset_function\n",
    "graph_preprocessed_coords, adj_matrix, to_preprocess, global_scaler = graph_dataset_function(my_deepof_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will embed our data with deep clustering methods. The core idea of deep clustering is to embed our preprocessed data with a neural network and retrieve a set of embeddings per time point, each assigned to a cluster. If you have already trained a model, set **pre_trained=True**** instead of *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_script import train_model_function\n",
    "trained_model = train_model_function(my_deepof_project, graph_preprocessed_coords, adj_matrix, pre_trained=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_deepof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
